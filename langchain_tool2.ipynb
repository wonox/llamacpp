{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\onowa\\miniconda3\\envs\\vir_env\\lib\\site-packages\\langchain\\utils\\utils.py:159: UserWarning: WARNING! input is not default parameter.\n",
      "                input was transferred to model_kwargs.\n",
      "                Please confirm that input is what you intended.\n",
      "  warnings.warn(\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m  You should always know how to get from one place to another.\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I now know the final answer\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m It is important that we look at these issues when we are faced with them.\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m You should always know how to get from one place to another.\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The following topics are available for display:\n",
      "    - Topic 0\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The following topics are available for display:\\n    - Topic 0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.callbacks.manager import CallbackManagerForToolRun, AsyncCallbackManagerForToolRun\n",
    "from typing import Optional, Type, Callable\n",
    "from pydantic import Field\n",
    "import requests\n",
    "import json\n",
    "\n",
    "'''\n",
    "# APIキーをセット (変数名はLangChain側で決められています)\n",
    "from langchain.llms import OpenAI\n",
    "open_api_key = os.environ[\"openai_api_key\"]\n",
    "# 言語モデルを指定\n",
    "llm = OpenAI(temperature=0)\n",
    "'''\n",
    "from langchain.llms import LlamaCpp\n",
    "import os\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"llama.cpp/models/rinna-youri-7b-chat-q4_K_M.gguf\",\n",
    "    # model_path=r\"llama.cpp/cpp\\models\\llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    input={\n",
    "        \"max_tokens\": 32,\n",
    "        \"stop\": [\"System:\", \"User:\", \"Assistant:\", \"\\n\"],\n",
    "    },\n",
    "    verbose=True,\n",
    "    temperature=1,\n",
    "    n_ctx=2048\n",
    ")\n",
    "\n",
    "# WebサーバのAPIエンドポイント\n",
    "url = \"https://cir.nii.ac.jp/opensearch/all?q=%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB&count=5&sortorder=0&format=json\"\n",
    "#\"http://localhost:5000/api/\"\n",
    "\n",
    "# トピック一覧を取得するツール\n",
    "class ListTopicTool(BaseTool): # BaseToolクラスのサブクラスとして、クラスを自作\n",
    "    name = \"ListTopic\"\n",
    "    description = \"\"\"\n",
    "    JSONデータを受け取ってtitleをまとめてください\n",
    "    \"\"\"\n",
    "    \n",
    "    # エンドポイントにGETリクエストを送信\n",
    "    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        response = requests.get(url)   #  + \"list_topics\")\n",
    "        topics = response.json()\n",
    "        return topics\n",
    "    \n",
    "    # 非同期実行の関数も必須\n",
    "    async def _run(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -> str:\n",
    "        raise NotImplementedError(\"ListTopicTool does not support async\")\n",
    "\n",
    "        \n",
    "\n",
    "def _print_func(text: str) -> None:\n",
    "    print(\"\\n\")\n",
    "    print(text)\n",
    "\n",
    "# ユーザにトピック選択を促すツール\n",
    "class HumanInputTool(BaseTool):\n",
    "    name = \"Human\"\n",
    "    description = \"\"\"\n",
    "    If you have obtained topics or if the Human`s desired \"topic_name\" is ambiguous, \n",
    "    you have must ask a Human which topics they are interested in. \n",
    "    The input should be a question for the Human. \"\n",
    "    You can also consult a Human when you feel stuck or unsure about what to do next.\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # ユーザへの質問を表示する関数\n",
    "    prompt_func: Callable[[str], None] = Field(default_factory=lambda: _print_func)\n",
    "    # 入力を受け付ける関数\n",
    "    input_func: Callable = Field(default_factory=lambda: input)\n",
    "        \n",
    "    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        # ユーザへの質問を表示する\n",
    "        self.prompt_func(query)\n",
    "        # 入力を受け付ける関数を実行\n",
    "        return self.input_func()\n",
    "\n",
    "    # 非同期実行関数の定義は必須\n",
    "    async def _arun(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None,) -> str:\n",
    "        raise NotImplementedError(\"Human tool does not support async\")\n",
    "\n",
    "    \n",
    "# 指定トピックの内容を取得するツール\n",
    "class GetTopicTool(BaseTool):  # BaseToolクラスのサブクラスとして、クラスを自作\n",
    "    name = \"GetTopic\"\n",
    "    description = \"\"\"\n",
    "    Retrieve a specific topic requested by the user.\n",
    "    This functionality is used when the user is seeking the content of a particular topic, \n",
    "    such as when they specify a topic name like \"Cloud Expansion Strategy\" or when they ask a question like \"Tell me about skill development for young employees.\"\n",
    "    When executing this functionality, you need to provide input in the form of a dictionary with key-value pairs. \n",
    "    The key should be \"topic_name\" in Japanese and the value should be the specified topic name by the user or the extracted topic name from the user's query.\n",
    "    \"\"\"\n",
    "    \n",
    "    # エンドポイントにPOSTリクエストを送信\n",
    "    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        query = query.replace(\"'\", '\"').encode('utf-8')\n",
    "        query_dict= json.loads(query)\n",
    "        response = requests.post(url + \"get_topic\", query_dict)\n",
    "        topic_content = response.text\n",
    "        return topic_content\n",
    "        \n",
    "    # 非同期実行関数の定義は必須\n",
    "    async def _arun(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -> str:\n",
    "        raise NotImplementedError(\"ListTopicTool does not support async\")\n",
    "            \n",
    "            \n",
    "# ツールを設定\n",
    "tools = [\n",
    "    ListTopicTool()\n",
    "]\n",
    "prompt_func: Callable[[str], None] = Field(default_factory=lambda: _print_func)\n",
    "\n",
    "    # HumanInputTool(),\n",
    "    # GetTopicTool()\n",
    "\n",
    "# エージェントを定義\n",
    "# agent = initialize_agent(tools, OpenAI(temperature=0), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                         verbose=True,\n",
    "                         handle_parsing_errors=True\n",
    "                        )\n",
    "\n",
    "# エージェント起動\n",
    "agent.run(\"トピックの一覧を表示し、トピックの内容を把握したいです。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vir_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
