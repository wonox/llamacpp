{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-8c5e118d-9e7c-4635-8616-840241ad3418', 'object': 'text_completion', 'created': 1700977566, 'model': 'llama.cpp\\\\models\\\\rinna-youri-7b-chat-q4_K_M.gguf', 'choices': [{'text': \"[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\nWrite a story about llamas.Please answer in Japanese.[/INST]\", 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 145, 'completion_tokens': 0, 'total_tokens': 145}}\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "# プロンプトを記入\n",
    "prompt = \"\"\"[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "Write a story about llamas.Please answer in Japanese.[/INST]\"\"\"\n",
    "# ダウンロードしたModelをセット.\n",
    "llm = Llama(model_path=r\"llama.cpp\\models\\rinna-youri-7b-chat-q4_K_M.gguf\", n_gpu_layers=20)\n",
    "# 生成実行\n",
    "output = llm(\n",
    "    prompt,max_tokens=500,stop=[\"System:\", \"User:\", \"Assistant:\"],echo=True,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "魔王を倒して王都に凱 : 36\n",
      "魔法使いフリーレンら : 21\n",
      "10年間もの旅路を終 : 23\n",
      "1000年は軽く生き : 31\n",
      "その旅はきわめて短い : 21\n",
      "50年に一度降るとい : 30\n",
      "次回もそれを見る約束 : 32\n",
      "すっかり年老いたヒン : 24\n",
      "ハイターやアイゼンと : 26\n",
      "。まもなくヒンメルは : 16\n",
      "彼の葬儀でフリーレン : 28\n",
      "知ろうともしなかった : 38\n",
      "人間を知るためと魔法 : 31\n",
      "フリーレンは老い先短 : 19\n",
      "。ハイターは魔導書の : 39\n",
      "。その4年後、 : 7\n",
      "フリーレンは魔導書の : 16\n",
      "フェルンは1人前の魔 : 16\n",
      "。ハイターの最期を看 : 28\n",
      "さらにその後、\\nアイ : 26\n",
      "彼の協力によりフリー : 35\n",
      "死者の魂と対話できる : 38\n",
      "。かつての魔王城の所 : 26\n",
      "ヒンメルとの再会を目 : 19\n",
      "。そしてアイゼンの弟 : 29\n",
      "フリーレン達の旅は続 : 12\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: oshizo/sbert-jsnli-luke-japanese-base-lite\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "c:\\Users\\onowa\\miniconda3\\envs\\vir_env\\lib\\site-packages\\langchain\\utils\\utils.py:159: UserWarning: WARNING! input is not default parameter.\n",
      "                input was transferred to model_kwargs.\n",
      "                Please confirm that input is what you intended.\n",
      "  warnings.warn(\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "A1:   彼女は友達を訪ねる旅を続けており、同じくらい年老いたヒンメルに会いました。\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from typing import Any\n",
    "\n",
    "# ログレベルの設定\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG, force=True)\n",
    "\n",
    "# LangChain用に句読点で分割してくれるTextSplitterを作ってみた\n",
    "# https://www.sato-susumu.com/entry/2023/04/30/131338\n",
    "class JapaneseCharacterTextSplitter(RecursiveCharacterTextSplitter):\n",
    "    def __init__(self, **kwargs: Any):\n",
    "        separators = [\"\\n\\n\", \"\\n\", \"。\"]  #, \"、\", \" \", \"、\",\"　\"]\n",
    "        super().__init__(separators=separators, **kwargs)\n",
    "\n",
    "# ドキュメントの読み込み\n",
    "# with open(\"bocchi_en.txt\") as f:\n",
    "with open(\"frielen_ja.txt\") as f:\n",
    "    test_all = f.read()\n",
    "\n",
    "# チャンクの分割\n",
    "# chunk_size が長すぎると時間がかかる、短すぎると（5以下）間違った回答になりやすい.40くらいがいいかも。\n",
    "japanese_spliter = JapaneseCharacterTextSplitter(\n",
    "    chunk_size=40,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "texts = japanese_spliter.split_text(test_all)\n",
    "\n",
    "'''\n",
    "# spacyはそのままだと日本語は分割できない \n",
    "text_splitter = SpacyTextSplitter(\n",
    "    chunk_size=1000  # チャンクのトークン数\n",
    ")\n",
    "texts = text_splitter.split_text(test_all)\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=300,  # チャンクの最大文字数\n",
    "#     chunk_overlap=20,  # オーバーラップの最大文字数\n",
    "# )\n",
    "# texts = text_splitter.split_text(test_all)\n",
    "'''\n",
    "\n",
    "# チャンクの確認\n",
    "print(len(texts))\n",
    "for text in texts:\n",
    "    print(text[:10].replace(\"\\n\", \"\\\\n\"), \":\", len(text))\n",
    "\n",
    "\n",
    "# インデックスの作成\n",
    "index = FAISS.from_texts(\n",
    "    texts=texts,\n",
    "    # embedding=HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\"),\n",
    "    # embedding=HuggingFaceEmbeddings(model_name=\"sonoisa/sentence-bert-base-ja-mean-tokens-v2\"),\n",
    "    embedding = HuggingFaceEmbeddings(model_name = \"oshizo/sbert-jsnli-luke-japanese-base-lite\", encode_kwargs={\"normalize_embeddings\":True })\n",
    ")\n",
    "index.save_local(\"storage\")\n",
    "\n",
    "# インデックスの読み込み\n",
    "# index = FAISS.load_local(\n",
    "#    \"storage\", HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
    "# )\n",
    "\n",
    "# LLMの準備\n",
    "# llm = OpenAI(temperature=0, verbose=True)\n",
    "# LlamaCpp(model_path=model, verbose=True, n_ctx=2048)\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"llama.cpp/models/rinna-youri-7b-chat-q4_K_M.gguf\",\n",
    "    # model_path=r\"llama.cpp/cpp\\models\\llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    input={\n",
    "        \"max_tokens\": 32,\n",
    "        \"stop\": [\"System:\", \"User:\", \"Assistant:\", \"\\n\"],\n",
    "    },\n",
    "    verbose=True,\n",
    "    temperature=1,\n",
    "    n_ctx=2048\n",
    ")\n",
    "\n",
    "# 質問応答チェーンの作成\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=index.as_retriever(search_kwargs={\"k\": 4}),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 質問応答チェーンの実行\n",
    "print(\"A1:\", qa_chain.run(\"フリーレンはどういう人ですか?\"))\n",
    "# print(\"A2:\", qa_chain.run(\"ハイターはどういう人ですか?\"))\n",
    "# print(\"A1:\", qa_chain.run(\"What kind of person is Hitori Goto?\"))\n",
    "# print(\"A2:\", qa_chain.run(\"What instrument is Hitori Goto good at?\"))\n",
    "# print(\"A3:\", qa_chain.run(\"What did Hitori Goto do at the school festival?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: intfloat/multilingual-e5-large\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu\n",
      "<langchain.vectorstores.faiss.FAISS object at 0x000001B3360E8D60>\n"
     ]
    }
   ],
   "source": [
    "# https://qiita.com/shimajiroxyz/items/91556e85254d5fff7f48\n",
    "# 【langchain】ベクトルストアの保存と読込方法を整理する(save_local vs pickle)\n",
    "# インデックスの読み込み\n",
    "index = FAISS.load_local(\n",
    "    \"storage\", HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
    ")\n",
    "print(index)\n",
    "# print(index.similarity_search_with_score(\"こんにちは\"))\n",
    "# print(\"A1:\", qa_chain.run(\"フリーレンはどういう人ですか?\"))\n",
    "# print(\"A2:\", qa_chain.run(\"ハイターはどういう人ですか?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain.vectorstores.faiss.FAISS object at 0x000001B3360E8D60>\n"
     ]
    }
   ],
   "source": [
    "print(index)\n",
    "# print(index.similarity_search_with_score(\"フリーレン\", k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ヒンメルは何をしましたか？ :   旅をする\n"
     ]
    }
   ],
   "source": [
    "# A2 = \"ハイターはどういう人ですか?\"\n",
    "A2 = \"ヒンメルは何をしましたか？\"\n",
    "print(A2, \":\", qa_chain.run(A2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zenn.dev/octu0/scraps/4f26b8e6a170c5\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "model_path = r\".\\models\\rinna-youri-7b-chat-q4_K_M.gguf\"\n",
    "\n",
    "stop = [\n",
    "    '\\nシステム: ',\n",
    "    '\\n\\tシステム: ',\n",
    "    '</s>'\n",
    "]\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    n_ctx=2048,\n",
    "    max_tokens=1024,\n",
    "    seed=1,\n",
    "    stop=stop,\n",
    "    f16_kv=True,\n",
    "    verbose=True,\n",
    ")\n",
    "prompt=\"\"\"ユーザー: 日本のおすすめの観光地を教えてください。<NL>\n",
    "システム: どの地域の観光地が知りたいですか？<NL>\n",
    "ユーザー: 渋谷の観光地を教えてください。<NL>\n",
    "システム:\"\"\"\n",
    "\n",
    "r = llm(prompt)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
