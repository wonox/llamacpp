{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT\n",
    "必要パッケージ\n",
    "- torch>=1.3.1\n",
    "- torchvision>=0.4.2\n",
    "- transformers>=2.5.0\n",
    "- tensorboard>=1.14.0\n",
    "- tensorboardX==1.8\n",
    "- scikit-learn>=0.21.0\n",
    "- mecab-python3\n",
    "\n",
    "# LINE-DistilBERT-Japanese\n",
    "Requirements\n",
    "- fugashi\n",
    "- sentencepiece\n",
    "- unidic-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\onowa\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'DistilBertJapaneseTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[-0.1122, -0.7709, -0.3854,  ...,  0.1166,  0.1344,  0.5023],\n",
      "         [ 0.0462,  0.1844, -0.2837,  ..., -0.3986,  0.0677,  0.2385],\n",
      "         [-0.5617, -0.1166,  0.0131,  ...,  0.1477,  0.2677, -0.1689],\n",
      "         ...,\n",
      "         [ 0.3022, -0.3256, -0.1269,  ...,  0.1317, -0.0817,  0.8145],\n",
      "         [ 0.1534, -0.5966, -0.3891,  ...,  0.7061, -0.0243,  0.5865],\n",
      "         [ 0.1918,  0.0993, -0.1755,  ..., -1.3809,  0.0155,  0.0842]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), hidden_states=(tensor([[[ 2.4789e-02, -1.9181e-01, -1.5257e-02,  ..., -2.0408e-01,\n",
      "           1.6245e-03, -1.2560e-01],\n",
      "         [ 9.7119e-01,  3.2164e-01, -1.2670e+00,  ..., -5.8727e-01,\n",
      "           4.0634e-01,  1.1144e+00],\n",
      "         [ 4.4050e-02, -1.6165e+00, -2.6058e-01,  ...,  3.1198e-01,\n",
      "           1.1372e+00,  3.7232e-02],\n",
      "         ...,\n",
      "         [ 1.4633e-01,  5.6657e-01,  8.7498e-01,  ..., -8.2278e-01,\n",
      "          -1.1253e+00,  3.0118e-01],\n",
      "         [ 8.6979e-01, -4.8372e-01, -2.3879e-01,  ...,  3.3803e-01,\n",
      "          -3.7104e-01, -3.4207e-01],\n",
      "         [ 2.7438e-02, -1.2145e-01, -1.0260e-03,  ..., -3.1155e-01,\n",
      "          -3.2899e-01,  8.4806e-03]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0216, -0.0074, -0.0241,  ..., -0.0434,  0.0082, -0.0123],\n",
      "         [ 0.5124,  0.3597, -0.8433,  ..., -0.1690, -0.0712,  0.4248],\n",
      "         [-0.0715, -0.8400, -0.3426,  ...,  0.4906,  0.5528, -0.1821],\n",
      "         ...,\n",
      "         [ 0.2612,  0.5604,  0.4190,  ..., -0.4851, -0.4884, -0.2160],\n",
      "         [ 0.5002, -0.0470, -0.1507,  ...,  0.9329, -0.4591, -0.6701],\n",
      "         [-0.0016,  0.0120,  0.0715,  ..., -0.0229,  0.0373,  0.0520]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0626, -0.2457,  0.1211,  ..., -0.2591,  0.1368,  0.0299],\n",
      "         [ 0.4454,  0.5190, -0.4018,  ..., -0.2815,  0.0648,  0.4724],\n",
      "         [ 0.1584, -0.8153, -0.2606,  ...,  0.3071,  0.6690, -0.2430],\n",
      "         ...,\n",
      "         [ 0.0370,  0.5363,  0.1225,  ..., -0.3802, -0.1628, -0.1714],\n",
      "         [ 0.2740, -0.0916, -0.0380,  ...,  0.6200, -0.1927, -0.1141],\n",
      "         [-0.0009, -0.0074, -0.0022,  ..., -0.0376,  0.0246,  0.0061]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0199, -0.3915, -0.0192,  ..., -0.2673,  0.1977,  0.1394],\n",
      "         [ 0.5144,  0.4457, -0.2542,  ..., -0.4392,  0.0180,  0.6021],\n",
      "         [ 0.0864, -0.9560, -0.0610,  ...,  0.1844,  0.1593, -0.1771],\n",
      "         ...,\n",
      "         [-0.0361,  0.6350,  0.0265,  ..., -0.3885, -0.0217,  0.2507],\n",
      "         [ 0.3874, -0.3332, -0.1032,  ...,  0.3528, -0.1252,  0.2531],\n",
      "         [-0.0124,  0.0104, -0.0175,  ...,  0.0059,  0.0114,  0.0067]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.2550, -0.5377,  0.1978,  ..., -0.0486,  0.5304, -0.0818],\n",
      "         [ 0.4257,  0.4421, -0.2333,  ..., -0.2888,  0.1586,  0.3616],\n",
      "         [-0.2395, -0.4638,  0.1813,  ...,  0.3520,  0.5677, -0.0822],\n",
      "         ...,\n",
      "         [ 0.0592,  0.2180,  0.3734,  ..., -0.5209,  0.1839,  0.4596],\n",
      "         [ 0.2654, -0.0322,  0.2075,  ...,  0.3668,  0.2778,  0.2091],\n",
      "         [-0.0258, -0.0454, -0.0273,  ..., -0.0590,  0.0065,  0.0199]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0950, -0.6581,  0.1399,  ...,  0.0700,  0.3822,  0.3850],\n",
      "         [ 0.2606,  0.1128, -0.2219,  ..., -0.3193,  0.1059,  0.2657],\n",
      "         [-0.8088, -0.4126,  0.1269,  ...,  0.2889,  0.5139, -0.3941],\n",
      "         ...,\n",
      "         [ 0.5557,  0.1186,  0.2920,  ..., -0.3068,  0.1576,  0.7078],\n",
      "         [ 0.4990, -0.3097, -0.0475,  ...,  0.6221,  0.3027,  0.6963],\n",
      "         [-0.0163, -0.0317, -0.0423,  ..., -0.0523, -0.0101, -0.0079]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.1122, -0.7709, -0.3854,  ...,  0.1166,  0.1344,  0.5023],\n",
      "         [ 0.0462,  0.1844, -0.2837,  ..., -0.3986,  0.0677,  0.2385],\n",
      "         [-0.5617, -0.1166,  0.0131,  ...,  0.1477,  0.2677, -0.1689],\n",
      "         ...,\n",
      "         [ 0.3022, -0.3256, -0.1269,  ...,  0.1317, -0.0817,  0.8145],\n",
      "         [ 0.1534, -0.5966, -0.3891,  ...,  0.7061, -0.0243,  0.5865],\n",
      "         [ 0.1918,  0.0993, -0.1755,  ..., -1.3809,  0.0155,  0.0842]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)), attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"line-corporation/line-distilbert-base-japanese\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"line-corporation/line-distilbert-base-japanese\")\n",
    "\n",
    "sentence = \"LINE株式会社で[MASK]の研究・開発をしている。\"\n",
    "print(model(**tokenizer(sentence, return_tensors=\"pt\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\onowa\\miniconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.25.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading config.json: 100%|██████████| 629/629 [00:00<00:00, 39.8kB/s]\n",
      "c:\\Users\\onowa\\miniconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\onowa\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading model.safetensors: 100%|██████████| 268M/268M [00:10<00:00, 25.5MB/s] \n",
      "Downloading tokenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<?, ?B/s]\n",
      "Downloading vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.27MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9998\n",
      "label: NEGATIVE, with score: 0.5309\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline \n",
    "classifier = pipeline('sentiment-analysis') \n",
    "results = classifier([\"We are very happy to show you the 🤗 Transformers library.\", \n",
    "           \"We hope you don't hate it.\"]) \n",
    "for result in results: \n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'DistilBertJapaneseTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.27118006348609924, 'token': 4761, 'token_str': 'ソフトウェア', 'sequence': '国立 情報 学 研究 所 で ソフトウェア の 研究 ・ 開発 を し て いる 。'}, {'score': 0.15745429694652557, 'token': 4264, 'token_str': 'コンピュータ', 'sequence': '国立 情報 学 研究 所 で コンピュータ の 研究 ・ 開発 を し て いる 。'}, {'score': 0.08551976084709167, 'token': 1135, 'token_str': 'システム', 'sequence': '国立 情報 学 研究 所 で システム の 研究 ・ 開発 を し て いる 。'}, {'score': 0.056933190673589706, 'token': 9271, 'token_str': 'データベース', 'sequence': '国立 情報 学 研究 所 で データベース の 研究 ・ 開発 を し て いる 。'}, {'score': 0.025575386360287666, 'token': 6050, 'token_str': 'ロボット', 'sequence': '国立 情報 学 研究 所 で ロボット の 研究 ・ 開発 を し て いる 。'}]\n"
     ]
    }
   ],
   "source": [
    "# Pipelinesによるモデル推論を試す\n",
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='line-corporation/line-distilbert-base-japanese', trust_remote_code=True)\n",
    "print(unmasker(\"国立情報学研究所で[MASK]の研究・開発をしている。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.5628989338874817, 'start': 514, 'end': 531, 'answer': 'COVID-19 pandemic'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa = pipeline(\"question-answering\")\n",
    "olympic_wiki_text = \"\"\"\n",
    "The 2020 Summer Olympics (Japanese: 2020年夏季オリンピック, Hepburn: Nisen Nijū-nen Kaki Orinpikku), officially the Games of the XXXII Olympiad (Japanese: 第32回オリンピック競技大会, Hepburn: Dai Sanjūni-kai Orinpikku Kyōgi Taikai), and also known as Tokyo 2020 (東京2020, Tōkyō ni-zero-ni-zero[2]), is an upcoming international multi-sport event scheduled to be held from 23 July to 8 August 2021 in Tokyo, Japan. Formerly scheduled to take place from 24 July to 9 August 2020, the event was postponed in March 2020 as a result of the COVID-19 pandemic, and will not allow international spectators.[3][4] Despite being rescheduled for 2021, the event retains the Tokyo 2020 name for marketing and branding purposes.[5] This is the first time that the Olympic Games have been postponed and rescheduled, rather than cancelled.[6]\n",
    "Tokyo was selected as the host city during the 125th IOC Session in Buenos Aires, Argentina, on 7 September 2013.[7] The 2020 Games will mark the second time that Japan has hosted the Summer Olympic Games, the first being also in Tokyo in 1964, making this the first city in Asia to host the Summer Games twice. Overall, these will be the fourth Olympic Games to be held in Japan, which also hosted the Winter Olympics in 1972 (Sapporo) and 1998 (Nagano). Tokyo was also scheduled to host the 1940 Summer Olympics but pulled out in 1938. The 2020 Games will also be the second of three consecutive Olympics to be held in East Asia, the first being in Pyeongchang County, South Korea in 2018, and the next in Beijing, China in 2022.\n",
    "The 2020 Games will see the introduction of new competitions including 3x3 basketball, freestyle BMX, and madison cycling, as well as further mixed events. Under new IOC policies, which allow the host organizing committee to add new sports to the Olympic program to augment the permanent core events, these Games will see karate, sport climbing, surfing, and skateboarding make their Olympic debuts, as well as the return of baseball and softball for the first time since 2008.[8]\n",
    "\"\"\"\n",
    "print(qa(question=\"What caused Tokyo Olympic postponed?\", context=olympic_wiki_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "qa = pipeline('question-answering', model='line-corporation/line-distilbert-base-japanese', trust_remote_code=True)\n",
    "# qa = pipeline(\"question-answering\")\n",
    "con_text = \"\"\"\n",
    "「2030 デジタル・ライブラリー」推進に関する検討会の設置について\n",
    "令和５年４月４日\n",
    "文部科学省研究振興局\n",
    "１ 設置の目的\n",
    "令和５年１月に「オープンサイエンス時代における今後の大学図書館の在り方検討部会」で取りまとめられた「審議のまとめ」において、大学図書館は、2030 年度を目\n",
    "途に「デジタル・ライブラリー」を実現するものと位置付けている。「『2030 デジタル・ライブラリー』推進に関する検討会」は、「審議のまとめ」に掲げられた構想を具体\n",
    "化するとともに、学術情報流通を推進する際の課題等を整理、検討する場として設置する。\n",
    "２ 検討内容\n",
    "・「審議のまとめ」における課題の整理、具体的な取組の検討\n",
    "・上記を実施する際に必要な調査・分析\n",
    "・その他、学術情報流通に関すること\n",
    "\"\"\"\n",
    "print(qa(question=\"デジタル・ライブラリーとは?\", context=con_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "model_name = \"line-corporation/line-distilbert-base-japanese\"\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name, trust_remote_code=True)\n",
    "QA_input = {\n",
    "    'question': \"デジタル・ライブラリーとは何ですか？\",\n",
    "    'context': \"\"\"\n",
    "    令和５年１月に「オープンサイエンス時代における今後の大学図書館の在り方検討部会」で取りまとめられた「審議のまとめ」において、大学図書館は、2030 年度を目\n",
    "途に「デジタル・ライブラリー」を実現するものと位置付けている。「『2030 デジタル・ライブラリー』推進に関する検討会」は、「審議のまとめ」に掲げられた構想を具体\n",
    "化するとともに、学術情報流通を推進する際の課題等を整理、検討する場として設置する。\n",
    "\"\"\"\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "print(res)\n",
    "print('答え: ' + res['answer'])\n",
    "\n",
    "# 10.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join, dirname\n",
    "from dotenv import load_dotenv\n",
    "env_key = ''\n",
    "load_dotenv(verbose=True)\n",
    "\n",
    "\n",
    "dotenv_path = '.env'\n",
    "# load_dotenv(dotenv_path)\n",
    "load_dotenv(dotenv_path, override=True)\n",
    "\n",
    "# env_key = os.environ.get(\"ENV_KEY\")\n",
    "env_key = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "# print(env_key)\n",
    "# DSN = os.environ.get(\"DB_NAME\")\n",
    "# USN = os.environ.get(\"USER_NAME\")\n",
    "# PWD = os.environ.get(\"PASSWORD\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＃ Sentence BERTをFine TuningしてFAQを類似文書検索してみる\n",
    " https://acro-engineer.hatenablog.com/entry/2023/01/16/120000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Authorization': 'Bearer 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'}\n",
      "[0.6712043285369873, 0.9685124158859253, 0.26977112889289856]\n"
     ]
    }
   ],
   "source": [
    "# プログラムからモデルを呼び出すことができる推論API\n",
    "# TOKENがいる\n",
    "import json\n",
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
    "bearer_key = 'Bearer ' + env_key\n",
    "# headers = {\"Authorization\": \"Bearer YOUR_TOKEN\"}\n",
    "# headers = {\"Authorization\": \"Bearer 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\"}\n",
    "headers = {\"Authorization\": bearer_key}\n",
    "print(headers)\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "data = query(\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"source_sentence\": \"That is a happy person\",\n",
    "             \"sentences\": [\n",
    "                \"That is a happy dog\",\n",
    "                \"That is a very happy person\",\n",
    "                \"Today is a sunny day\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summerizarion\n",
    "https://huggingface.co/transformers/v4.8.2/main_classes/pipelines.html?highlight=summarization#transformers.SummarizationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TypeError: The current model class (DistilBertForMaskedLM) is not compatible with `.generate()`, as it doesn't have a language model head.\n",
    "'''\n",
    "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
    "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
    "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
    "The class this function is called from is 'DistilBertJapaneseTokenizer'.\n",
    "The model 'DistilBertForMaskedLM' is not supported for summarization. Supported models are [...\n",
    "'''\n",
    "\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "model_name = \"line-corporation/line-distilbert-base-japanese\"\n",
    "summarizer = pipeline('summarization', model=model_name, tokenizer=model_name, trust_remote_code=True)\n",
    "# summarizer = pipeline(\"summarization\")\n",
    "# summarizer(\"An apple a day, keeps the doctor away\", min_length=5, max_length=20)\n",
    "# prefix = \"summarize: \"\n",
    "\n",
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "... A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "... Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "... In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "... Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "... 2010 marriage license application, according to court documents.\n",
    "... Prosecutors said the marriages were part of an immigration scam.\n",
    "... On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "... After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "... Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "... All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "... Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "... Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "... The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "... Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "... Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "... If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "... \"\"\"\n",
    "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by inference API: Model elyza/ELYZA-japanese-Llama-2-7b-instruct time out",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7448/2069467042.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# question = \"Who won the FIFA World Cup in the year 1994? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mllm_chain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\onowa\\miniconda3\\lib\\site-packages\\langchain\\chains\\base.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m             return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[0;32m    506\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m             ]\n",
      "\u001b[1;32mc:\\Users\\onowa\\miniconda3\\lib\\site-packages\\langchain\\chains\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
      "\u001b[1;32mc:\\Users\\onowa\\miniconda3\\lib\\site-packages\\langchain\\chains\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m             outputs = (\n\u001b[1;32m--> 304\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\onowa\\miniconda3\\lib\\site-packages\\langchain\\chains\\llm.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     ) -> Dict[str, str]:\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\onowa\\miniconda3\\lib\\site-packages\\langchain\\chains\\llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseLanguageModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             return self.llm.generate_prompt(\n\u001b[0m\u001b[0;32m    121\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\onowa\\miniconda3\\lib\\site-packages\\langchain\\llms\\base.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m     ) -> LLMResult:\n\u001b[0;32m    506\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     async def agenerate_prompt(\n",
      "\u001b[1;32mc:\\Users\\onowa\\miniconda3\\lib\\site-packages\\langchain\\llms\\base.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    654\u001b[0m                 )\n\u001b[0;32m    655\u001b[0m             ]\n\u001b[1;32m--> 656\u001b[1;33m             output = self._generate_helper(\n\u001b[0m\u001b[0;32m    657\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m             )\n",
      "\u001b[1;32mc:\\Users\\onowa\\miniconda3\\lib\\site-packages\\langchain\\llms\\base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    542\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 544\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    545\u001b[0m         \u001b[0mflattened_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflattened_output\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflattened_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\onowa\\miniconda3\\lib\\site-packages\\langchain\\llms\\base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             output = (\n\u001b[1;32m--> 531\u001b[1;33m                 self._generate(\n\u001b[0m\u001b[0;32m    532\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\onowa\\miniconda3\\lib\\site-packages\\langchain\\llms\\base.py\u001b[0m in \u001b[0;36m_generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1051\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m             text = (\n\u001b[1;32m-> 1053\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1054\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m                 \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\onowa\\miniconda3\\lib\\site-packages\\langchain\\llms\\huggingface_hub.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"error\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Error raised by inference API: {response['error']}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"text-generation\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[1;31m# Text generation return includes the starter text.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error raised by inference API: Model elyza/ELYZA-japanese-Llama-2-7b-instruct time out"
     ]
    }
   ],
   "source": [
    "# 5分かかってタイムアウト ValueError: Error raised by inference API: Model google/flan-t5-xl time out\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "# LLMChain \n",
    "from langchain.chains import LLMChain\n",
    "# export HUGGINGFACEHUB_API_TOKEN=XXXX は dotenv で環境変数に。\n",
    "repo_id = \"elyza/ELYZA-japanese-Llama-2-7b-instruct\"\n",
    "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\":0, \"max_length\":64})\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"日本語はどのような言語ですか\"\n",
    "# question = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 139/139 [00:00<00:00, 303kB/s]\n",
      "Downloading spiece.model: 100%|██████████| 812k/812k [00:00<00:00, 978kB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 2.42M/2.42M [00:01<00:00, 2.41MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 43.0/43.0 [00:00<00:00, 9.50kB/s]\n",
      "The model 'DistilBertModel' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlp-waseda/gpt2-small-japanese\", trust_remote_code=True)\n",
    "# line-corporation/line-distilbert-base-japanese\n",
    "model = AutoModel.from_pretrained(\"line-corporation/line-distilbert-base-japanese\")\n",
    "\n",
    "#ーーー\n",
    "# model_id = \"gpt2\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "# ーーー\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"Who won the FIFA World Cup in the year 1994? \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '今日 の 晩御飯 は カレー でした 。   ・ 5 つ 星 リゾート の コンドミニアム です 。 全 室 で 空調 完備 で 、'},\n",
       " {'generated_text': '今日 の 晩御飯 は カレー で カレー ラーメン でした ♪ おいしかった です 。  今年 も 残す ところ あと 一 ヶ月 です ね 。 来'},\n",
       " {'generated_text': '今日 の 晩御飯 は カレー ライス ( これ は 別の 人 が 作り ました )   ※ 画像 を クリック する と 、 商品 詳細 ページ 、'},\n",
       " {'generated_text': '今日 の 晩御飯 は カレー に しよう と 考えて い ました が 、 カレー の 味 が 濃くて ちょっと 味 が 薄 そうな 気 が して 、'},\n",
       " {'generated_text': '今日 の 晩御飯 は カレー を いただき ました 。  今日 は 、 わざわざ 来て いただいて 色々 と お 話し さ せて いただき ました'}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='nlp-waseda/gpt2-small-japanese')\n",
    "set_seed(42)\n",
    "generator(\"今日 の 晩御飯 は カレー\", max_length=30, do_sample=True, pad_token_id=2, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
