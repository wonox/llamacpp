{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain.llms as llms\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.llms import LlamaCpp\n",
    "import pandas as pd\n",
    "import os\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\onowa\\miniconda3\\envs\\vir_env\\lib\\site-packages\\langchain\\utils\\utils.py:159: UserWarning: WARNING! input is not default parameter.\n",
      "                input was transferred to model_kwargs.\n",
      "                Please confirm that input is what you intended.\n",
      "  warnings.warn(\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=\"llama.cpp/models/rinna-youri-7b-chat-q4_K_M.gguf\",\n",
    "    # model_path=r\"llama.cpp/cpp\\models\\llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    input={\n",
    "        \"max_tokens\": 32,\n",
    "        \"stop\": [\"System:\", \"User:\", \"Assistant:\", \"\\n\"],\n",
    "    },\n",
    "    verbose=True,\n",
    "    temperature=1,\n",
    "    n_ctx=2048\n",
    ")\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pdf(chain, path):\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages = loader.load_and_split()\n",
    "    return chain.run(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "def summarize_text(chain, path):\n",
    "    # loader = PyPDFLoader(path)\n",
    "    # pages = loader.load_and_split()\n",
    "    # return chain.run(pages)\n",
    "    loader = TextLoader(path)\n",
    "    data = loader.load()\n",
    "    # print(data)\n",
    "    return chain.run(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n1. ヒンメルは魔王との決戦で仲間を失い、50年前パーティを解散させた。\\n2. フリーレンが他界してからもう少し、今回再会することになった4人の勇者は魔法や知識を学ぶ時間が短くなりつつあった。\\n3. 10年後、フェルンは大きな仕事で自分の身元を明かす準備をしているハイターと出会う。'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarize_pdf(chain, r\"C:\\Users\\onowa\\OneDrive\\Documents\\The story follows elven mage Frieren.pdf\") \n",
    "# r\"C:\\Users\\onowa\\OneDrive\\Documents\\P-010_hikae_TZ-00441267_20230709.pdf\")\n",
    "summarize_text(chain, r\"C:\\Users\\onowa\\OneDrive\\Documents\\フリーレンあらすじ.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/DrEntropy/PDFSummary/blob/2fe7cd1d1d15f6e3f208d0705aff11ea995e56a8/PDF_summary.ipynb#L20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\\n\",  # セパレータ\n",
    "    chunk_size = 11,  # チャンクの文字数\n",
    "    chunk_overlap = 0,  # チャンクオーバーラップの文字数\n",
    ")\n",
    "textlist = ['PFRの家庭向けロボ、大規模言語モデル経由で操作 生成コードはLuaを採用、将来はマルチモーダル型へ', 'LINEヤフーが人体CG向け生成AI技術 役割を指定して2人の相互動作を出力', '〔第１０１回〕\\u3000ＣＥＡＴＥＣ2023：大規模言語モデルＰＬａＭｏ、\\u3000ＭＮ−Ｃｏｒｅ、3Ｄ空間スキャン', 'ロボの大規模言語モデル応用が次のフェーズへ 動作計画の拘束条件をLLMで生成、学習データ不要', '類似文書検索と大規模言語モデルを用いた事故記録の解析']\n",
    "def summarize_list(chain, path):\n",
    "    chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "    # chain.run(docs)\n",
    "    return chain.run(path)\n",
    "# summarize_text(chain, textlist)\n",
    "texts = text_splitter.create_documents(textlist)\n",
    "summaries = [summarize_list(chain, file) for file in texts]\n",
    "summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"PFRの家庭向けロボ、大規模言語モデル経由で操作 生成コードはLuaを採用、将来はマルチモーダル型へ\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LINEヤフーが人体CG向け生成AI技術 役割を指定して2人の相互動作を出力\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"〔第１０１回〕　ＣＥＡＴＥＣ2023：大規模言語モデルＰＬａＭｏ、　ＭＮ−Ｃｏｒｅ、3Ｄ空間スキャン\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"ロボの大規模言語モデル応用が次のフェーズへ 動作計画の拘束条件をLLMで生成、学習データ不要\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"類似文書検索と大規模言語モデルを用いた事故記録の解析\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"\n",
      "\n",
      "Home Assistant, the digital assistant that PFR offers to consumers, has been redesigned and expanded to include a large language model.  The new design is based on LUA and will allow multiple languages in the future.\n",
      "\n",
      "\n",
      "\n",
      "LINEとYahooは、架空のキャラクターやエンティティが、生成AIモデルによって生成された2人の相互作用を表示する新しい技術を開発しました。  この研究は、架空のキャラクターである「Pony」と「Joe」について説明されており、両者が人間型コンピュータ生成されたキャラクターを使って相互作用を持っています。  この技術は、単一の入力フィールドと複数の入力レイヤーによる生成モジュールから構成されており、1つ以上の入力があり、相互作用を持った2人（またはグループ）に特化しています。  これによ\n",
      "\n",
      "\n",
      "\n",
      " A new application for large language models (LLMs) will allow people to create task plans that do not require any data, and which can adapt to changes in the environment.  LLMs have already been put to use planning complex operations like manufacturing and surgery, but this latest application is less restricted as it uses “generative pre-trained transformer” (GPT) language models to synthesize task plans on-the-fly based only on a description of the overall operation.\n",
      "\n",
      "“One could think of this technology as providing a ‘natural language interface to robots’,” says Ajay Kilara, a postdoc in Carnegie Mellon University (CMU)’s Robotics Institute who developed this new application with colleagues at CMU and Disney Research.\n",
      "\n",
      "  The accident report was found by similar document search and large language model.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "PFR announces its new digital assistant Home Assistant, which now features a large language model that can answer general questions (similar to Google Search or Bing) as well as create task plans based on natural language input.\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "# 分割した長文を書く文章ごとにDocumentオブジェクト化\n",
    "docs = [Document(page_content=t) for t in textlist]\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "output = chain(\n",
    "    inputs=docs,\n",
    "    return_only_outputs=True,\n",
    ")[\"output_text\"]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PFR announces its new digital assistant Home Assistant, which now features a large language model that can answer general questions (similar to Google Search or Bing) as well as create task plans based on natural language input.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vir_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
